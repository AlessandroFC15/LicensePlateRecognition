<?xml version="1.0" encoding="UTF-8"?>
<refentry version="5.0-subset Scilab" xml:id="ann_FF_grad_BP" xml:lang="en"
          xmlns="http://docbook.org/ns/docbook"
          xmlns:xlink="http://www.w3.org/1999/xlink"
          xmlns:svg="http://www.w3.org/2000/svg"
          xmlns:ns4="http://www.w3.org/1999/xhtml"
          xmlns:mml="http://www.w3.org/1998/Math/MathML"
          xmlns:db="http://docbook.org/ns/docbook">
  <info>
    <pubdate>$LastChangedDate: 2008-03-26 09:50:39 +0100 (mer., 26 mars 2008)
    $</pubdate>
  </info>

  <refnamediv>
    <refname>ann_FF_grad_BP</refname>

    <refpurpose>error gradient trough backpropagation</refpurpose>
  </refnamediv>

  <refsection>
    <title>CALLING SEQUENCE</title>

    <para>W = ann_FF_grad_BP(x,t,N,W[,c,af,err_deriv_y])</para>
  </refsection>

  <refsection>
    <title>PARAMETERS</title>

    <para>x Matrix of input patterns, one pattern per column.</para>

    <para> t Matrix of targets, one pattern per column. Each column have a
    correspondent column in x. </para>

    <para>N Row vector describing the number of neurons per layer. N(1) is the
    size of input pattern vector, N(size(N,'c')) is the size of output pattern
    vector (and also target). </para>

    <para>W The weight hypermatrix (initialized first trough
    ann_BP_init).</para>

    <para> c defines the threshold of error which is backpropagated: a error
    smaller that c (at one neuronal output) is rounded towards zero and thus
    not propagated. It have to be set to zero for exact calculation of
    gradient. This parameter is optional, default value 0. </para>

    <para>af Activation function and its derivative. Row vector of strings:
    af(1) name of activation function. af(2) name of derivative.</para>

    <para> Warning: given the activation function y=f(x), the derivative have
    to be expressed in terms of y, not x. This parameter is optional, default
    value is "['ann_log_activ', 'ann_d_log_activ']", i.e. logistic activation
    function and its derivative. </para>

    <para>err_deriv_y the name of error function derivative with respect to
    network outputs. This parameter is optional, default value is
    "ann_d_sum_of_sqr", i.e. the derivative of sum-of-squares.</para>
  </refsection>

  <refsection>
    <title>Description</title>

    <para>Returns the error gradient hypermatrix (it have the same layout as
    W) of a feedforward ANN, using the whole given training set. The algorithm
    used is standard backpropagation.</para>
  </refsection>

  <refsection>
    <title>EXAMPLES</title>

    <para>This function is used as a low level engine in other algorithms
    (thus the reason for existence of c parameter), e.g. see implementation of
    ann_FF_ConjugGrad function.</para>
  </refsection>

  <refsection>
    <title>See Also</title>

    <simplelist type="inline">
      <member><link linkend="ANN">ANN</link></member>

      <member><link linkend="ann_FF">ann_FF</link></member>

      <member><link linkend="ann_FF_init">ann_FF_init</link></member>
    </simplelist>
  </refsection>
</refentry>